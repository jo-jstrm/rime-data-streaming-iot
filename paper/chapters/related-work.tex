%!TEX root = ../main.tex
\section{Related Work}
\label{sec:related-work}
% \steffen{provide an overview sentence what will follow}
Rime combines ideas from multiple subfields. This section covers the related 
work, as well as the differentiating factors, towards envisioning and implementing Rime.

\textbf{Wireless Sensor Networks} TinyDB~\cite{madden2005tinydb} coined the term of Acquisitional Query Processing (ACQP) for WSNs. It is one of the earliest attempts to utilize the inherent ability of sensors of controlling \textit{where}, \textit{when}, and \textit{how frequently} to physically acquire values as part of TinyOS. In that context, TinyDB uses SRTs to address the question of \textit{where} to sample data and discard areas that are not relevant to the active query. In Rime, we extend SRTs by allowing the exchange and update of routing trees between nodes in different physical locations.

\textbf{In-Network Processing and Protocols} The sensor-cloud, an amalgamation of heterogeneous resources, utilizes different communication models depending on the location. On one hand, edge nodes tend to utilize a more relaxed approach to
connectivity, in order to handle movement and transitivity. On the other hand, edge gateways and cloud nodes assume robust interconnects. Existing work on WSNs lead to the creation of power-sensitive protocols. Trickle~\cite{levis2008emergence} aids synchronization of state in a distributed WSN. It performs a ``polite gossip'' between nodes. The protocol aims to quickly detect and resolve an inconsistent state within the network while performing as few message exchanges as possible. Trickle offers a  trade-off between quick adaption to changed state and maintaining a low messaging- and processing-load. However, the protocol assumes only a multi-hop communication style, which is not the case in Rime. The protocol is not fit for an Edge deployment as it cannot exploit the ability to target different physical locations from a gateway node. Rime accounts for non-broadcast communication in its core design. This way a lot of multi-hop communication is reduced and nodes can have better routing information available anytime.
%\steffen{elaborate more what you do is different and why Rime is better}

\textbf{Mobility-Aware Stream Processing} Mobility-Aware Stream Processing refers to Stream Processing Engines (SPEs) that process tasks on mobile nodes first. This is achieved by distributing workload among nodes or providing a unified
view on the available resources. Our work is similar to systems that focus on maximizing routing and path diversity.
Frontier~\cite{okeeffe2018frontier} is a stream processing engine for the IoT. Frontier utilizes a modified back-pressure routing algorithm for load balancing between nodes. Frontier deploys query operators across multiple nodes, thus replicating the dataflow graph. This enables parallel processing of data by sending batches of data to replicated operators on distinct nodes. This approach also helps in case of node failures, because the replication of operators enables re-routing. However, if a query path in Frontier becomes unavailable due to a node failure, the same path is not available for the system to use, even if it recovers. Therefore, Frontier addresses the challenge of failing nodes in the IoT only partially compared to the always-updating approach of Rime. In Rime, information propagate from the nodes themselves, which are also responsible for updating their list of connections. By propagating this list to other nodes, based on the tracking policy, Rime ensures the path will be available again after recovery.

Similarly to Frontier, R-MStorm~\cite{chao2020r} utilizes the notion of path diversity for data flowing from the sources in the edge to the sinks in the cloud. R-MStorm follows a weighted-link strategy, where it gives higher weights to links that have been known to be in ``good'' quality over time. R-MStorm does not provide any guarantees at the level of the source, while Rime allows for reconnection of nodes regardless of their type.
% \steffen{please add our CIDR and VLIOT paper from NES}

NebulaStream~\cite{zeuch2020nebulastream, zeuch2020vloitnebulastream} is an SPE whose goal is the integration of the heterogeneous hardware in an IoT deployment under the same runtime. NebulaStream exploits any hardware capabilities in order to further optimize queries and perform runtime re-optimizations. Rime can enhance the networking stack of NebulaStream, as NebulaStream does not contain the notion of SRTs. By doing so, Rime can make NebulaStream more robust under heterogeneous networking schemes and strengthen its mobile processing capabilities even under frequent failures.

% \textbf{TinyDB} is a stream processing engine (SPE) for WSNs based on TinyOS~\cite{levis_tinyos_2005} and unites several WSN-specific approaches to data management into one system\cite{madden2005tinydb}. It combines several techniques aimed at reducing latency, improving failure resistance, and increasing energy efficiency of WSNs under the umbrella-term Acquisitional Query Processing (ACQP). ACQP addresses the questions of \textit{where}, \textit{when}, and \textit{how of\-ten} data should be sampled. In that context, TinyDB uses SRTs to address the question of \textit{where} to sample data.

% Similar to TinyDB’s approach of reducing the number of message transmissions by using knowledge of where, i.e. at which node, a query applies, distributed window aggregation can also reduce network-load~\cite{benson_disco_2020}. We extend TinyDB’s approach to SRTs by adapting it to the fog. However, we do not consider other ACQP techniques of TinyDB for Rime. 

% \johannes{\textbf{Orleans} ?}

% \johannes{\textbf{Frontier} ?}
% \textbf{Network Awareness.} \steffen{this comes out of the nowhere and is not connected}\dimi{this can also be moved to a related work section and mention Frontier for all of this} \johannes{For added context: I think this line of thought here comes from the thesis, where I used a similar section to explain that Rime goes above using properties such as location and connection quality for building a routing tree and representing the network-topology. Rime can use categorical information such as hardware-architecture, elevation, sensor-reading-frequency,..., as basis for building an SRT, thereby creating different *views* on the network.}
% Network awareness refers to the loose coupling of the \textit{physical} and \textit{logical} network topologies. 
% % 
% A \textit{physical} topology refers to the nodes and their physical properties, such as location, velocity, or link quality. 
% A \textit{logical} topology is a the set of physical nodes that are available to the application and are able to communicate.
% The logical topology allows to enrich network information with application-specific details, e.g.: times a node replied to a query.
% Based on the logical topology, an application is able to make decisions, e.g., which nodes must be included into a query plan. 
% However, IoT topologies are dynamic because of device movement or hardware failures. 
% This dynamicity affects the paths between nodes and introduces partitions with ad-hoc subnets in the network.
% In general, fast and timely propagation of changes from the physical to the logical topology are necessary in order to keep up with the physical world.

% old content
% \section{old content}
% To the best of our knowledge, SRTs have been implemented only in Tiny\-DB for usage in WSNs. Given that, single features from TinyDB, such as pushdown of query-\-operators~\cite{madden_tinydb:_2005} have been applied to various areas of research~\cite{abadi_aurora_2003}, including distributed systems. There have been different research efforts into the use of actors for data stream and -storage engines. There is the concept of accessors, which use actors as the foundation for a communication interface between smart devices~\cite{brooks_component_2018}. Furthermore, the benefits of actors regarding distributed computing can be leveraged to enable modular, scalable, efficient, and consistent \textit{actor-relational} database systems~\cite{shah_actor-relational_2018}. Additionally, the challenge of modelling the fog topology, i.e. abstracting the physical world of nodes and sensors to a context-aware layer, is an open issue~\cite{wang_modeling_nodate}.

% Because of its relevance to our work, we explain the key characteristics of TinyDB in Section~\ref{TinyDB}. Further, we explain how a recent SPE for the fog has tackled the research challenges of FEC in Section~\ref{Frontier}. Because the exchange of state between nodes is necessary when using SRTs, we explain a selected protocol for this purpose in Section~\ref{State Consistency in Distributed Systems}. Additionally, we explain good practices in evaluation of distributed systms in Section~\ref{Obtaining Measurements}. 

% \section{TinyDB}\label{TinyDB}
% TinyDB is a SPE for WSNs and based on TinyOS~\cite{levis_tinyos_2005}. Building on previous work~\cite{madden_tag_2003}, TinyDB unites several WSN-specific approaches to data management into the term Acquisitional Query Processing~\cite{madden_tinydb:_2005}. More precisely, ACQP combines several techniques aimed at reducing latency, improving failure resistance, and increasing energy efficiency of WSNs. It addresses the questions of \textit{where}, \textit{when}, and \textit{how of\-ten} data should be sampled. TinyDB is implemented in nesC~\cite{gay_nesc_2003}, mainly because nesC enables static composition of a program with all function calls at compile-time. Thereby, nesC reduces the amount of dynamic memory that is needed during runtime~\cite{levis_tinyos_2006}. TinyDB’s data model consists of a single table \textit{sensors}, which has one row per measurement per instant in time. Sensor readings are only materialized when needed and reside on the respective nodes, thus the data collection is pull-based. The system has a SQL-like query language that provides features such as setting durations for queries, window-aggregation, and events that trigger consecutive action. TinyDB employs a cost-based query optimizer that notably takes into account sampling costs of different attributes (\textit{exemplary aggregate pushdown}). Further, it can optimize execution of event-based queries by buffering readings of an event-based query for usage at the next event (\textit{event query batching}). We discussed how TinyDB uses SRTs to efficiently disseminate queries into the network in Section~\ref{Semantic Routing Trees}. To further reduce energy consumption, nodes snoop on other nodes’ messages. This means that a node listens to a broadcast message that is addressed to another node and uses the information for its own purposes. Nodes can use snooped messages to improve parent selection by listening to a neighboring node’s parent selection message and determining if that parent would be a good fit for themselves. In the same way, nodes can snoop on query results, especially in exemplary aggregations such as \textit{min} or \textit{max}~\cite{madden_tag_2003}. The snooped aggregation might have values that make sending the node’s own aggregate superfluous.

% Besides TinyDB, there have been several research projects on stream processing in WSNs such as Cougar~\cite{yao_cougar_2002, demers_cougar_2003}. We extend TinyDB’s approach to SRTs by adapting it to the fog. However, we do not consider other ACQP techniques of TinyDB for the fog. Similar to TinyDB’s approach of reducing the number of message transmissions by using knowledge of where, i.e. at which node, a query applies, distributed window aggregation can also reduce network-load~\cite{benson_disco_2020}.

% \section{Frontier}\label{Frontier}
% Frontier~\cite{okeeffe_frontier:_2018} is a stream processing engine for the fog. The system utilizes a backpressure routing algorithm for load balancing between nodes. Frontier deploys query operators across multiple nodes, thus replicating the dataflow graph. This enables parallel processing of data by sending batches of data to replicated operators on distinct nodes. This approach also increases resiliency against node failures, because the replication of operators enables fast re-routing. Lastly, each node stores processed batches of data until a downstream node acknowledges processing of the same batch. In the case of a failure, nodes can send the unacknowledged batches again, thus enabling fast recovery and avoiding loss of data. However, if a query path in Frontier becomes unavailable due to a node-failure, the path cannot reactivate when the node reconnects. Therefore, Frontier only partially addresses the challenge of failing nodes in the fog.

% Our approach of using SRTs for building routing paths differs from Frontier’s, because we aim to enable reintegration of nodes into the SRT, after the node has recovered from its failure. In contrast, if in Frontier a routing path is unavailable due to node failure, it will not reactivate after reconnection. However, Frontier provides much a more sophisticated approach to query processing and load balancing than our work.

% \section{State Consistency in Distributed Systems}\label{State Consistency in Distributed Systems}

%   % \begin{table}
%   %   \begin{tabular}{ll}
%   %     Variable  & Description \\
%   %     \hline
%   %     $\tau$  & Interval duration, dynamically changing. \\
%   %     $\tau_{low}$ & Lower limit and initial value for $\tau$. \\
%   %     $\tau_{high}$ & Upper limit for $\tau$. \\
%   %     $t$ & Interval duration, randomly chosen from $]\frac{\tau}{2},\tau]$. \\
%   %     $k$ & Predefined threshold that determines if state is broadcast. \\
%   %     $c$ & Counter for consistent state received in current interval. \\
%   %   \end{tabular}
%   %   \caption{Overview of Trickle parameters and their meaning.}
%   %   \label{fig:trickle-table}
%   % \end{table}

%   If nodes possess and exchange state in a distributed system, there is the issue of achieving and maintaining state-consistency between all participants of the system. While there are are mechanisms such as publish/subscribe messaging patterns in IP-based networks, the issue becomes more delicate in WSNs. Here, nodes rely on broadcast and can communicate only within antenna-range. Because our SRTs originate from WSNs and nodes in a SRT must exchange state, i.e. their attribute ranges, maintaining a consistent state through the network is highly relevant. To highlight the challenge of maintenance of state consistency in distributed systems, we briefly explain a selected protocol for that purpose. While we have decided on a publish/subscribe pattern for state exchange (see Section~\ref{Semantic Routing in the Fog}), our initial version of SRTs used Trickle~\cite{levis_trickle_2004} as a protocol to reach eventual state consistency in the network. 

%   Trickle aids synchronization of state in a distributed system. It performs a 'polite gossip'~\cite{levis_emergence_2008} between nodes. The protocol aims to quickly detect and resolve an inconsistent state within the network while performing as few message exchanges as possible. Because both goals are opposed, a trade-off occurs between quick adaption to changed state and maintaining a low messaging- and processing-load. Lastly, the protocol aims to be scalable to dynamically changing network topologies.

%   In Trickle, the aforementioned trade-off is dynamic: depending on the current need, i.e either fast propagation of changes or low overhead, the frequency of message exchange between nodes changes. A node running Trickle maintains two timers. One timer fires every $\tau$, which increases over time when a node receives consistent state. It is initialized with $\tau_{low}$ and can increase to at most $\tau_{high}$. Both $\tau_{high}$ and $\tau_{low}$ are the same for each node and predefined for the whole system. The other timer fires after $t$, which is randomly chosen in the interval $]\tau/2,\tau]$. Whenever $\tau$ changes, Trickle adjusts $t$ accordingly.

%   The protocol works as follows: when receiving an updated state from another node, the receiver checks for consistency with its own state. If the local and received states are consistent, the receiver increments a counter $c$. After time $\tau$ has elapsed, an exponential backoff is performed by doubling $\tau$, up to a maximum of $\tau_{high}$, resetting $c$ to $0$, and choosing a new $t$. If the received and local states are inconsistent~\footnote{~We assume the received state is newer. For different use cases the way of determining which state is more recent and when two states are equal might differ.}, the node updates its own state with the received one, resets $\tau$ to $\tau_{low}$, and broadcasts its new state.

%   When $t$ elapses, which happens each interval because $t\in~]\tau/2,\tau]$, the node compares $c$ to a predefined value $k$. If $c<k$, the node broadcasts its state; otherwise the node does nothing and waits for either $\tau$ to expire or new state to arrive. The meaning of each variable is illustrated in~\ref{fig:trickle-table}.

%   By performing an exponential backoff with $\tau$ in times of consistency, Trickle quickly reduces its messaging overhead. On the other hand, in times of inconsistency the change of state disseminates quickly, because of the reset of $\tau$ to $\tau_{low}$ and the immediate broadcast of changes. Further, the aforementioned polite gossip, i.e. broadcast of a node's local state in times of consistency when the node has not received enough ($c<k$) states from other nodes, ensures a reasonable quick detection of inconsistencies. Additionally, the random choice of $t$ uniformly distributes the amount of times each node must broadcast its state across all participating nodes.

%   The main factors influencing the performance of Trickle are package loss, network density, number of nodes $n$, and synchronization of timer-intervals. The key metric for measuring Trickle's behaviour is the number of transmissions per interval. With the aforementioned choice of $t$,~\cite{levis_trickle_2004} show that the number of transmissions scales with $O(\log(n))$ for lossy, unsynchronized, multi-hop networks. The choice of $t$ from interval $]\tau/2,\tau]$ is preferred over the naive approach of choosing from $]0,\tau]$ to avoid the \textit{short listen problem}~\cite{levis_trickle_2004}. It occurs if timer intervals are not synchronized between nodes. Because of overlapping intervals, when a node randomly chooses a small $t$, it does not have the chance to listen to transmissions from other nodes, leaving it no chance to receive messages and increasing $c$. This phenomenon increases the number of transmissions significantly. If the interval for $t$ is decreased, each node has a mandatory listening period and therefore the chance to receive transmissions from other nodes.

% \section{Obtaining Measurements}\label{Obtaining Measurements}
%     There are several pitfalls in benchmarking distributed systems which lead to wrong results~\cite{karimov_benchmarking_2018}. Therefore in this section we precisely define the terms \textit{latency} and \textit{throughput} of a system.
    
%     %\textbf{Processing-time Latency} is defined as \enquote{the interval between a tuple’s ingestion time (i.e., the time that the event has reached the input operator of the streaming system) and its emission time from the SUT output operator}~\cite{karimov_benchmarking_2018}. All delays that occur before ingestion are ignored.
    
%     %\textbf{Event-time Latency} is defined as \enquote{the interval between a tuple’s event-time and its emission time from the SUT output operator}~\cite{karimov_benchmarking_2018}, which most closely resembles the real-world behaviour.

%     Furthermore, ignoring processing-\-time la\-ten\-cy can lead to the \textit{coordinated omission} problem. Coordinated omission means that time which data spend waiting in a queue, before the tested systems starts processing them, is ignored. This is a problem, because, while not affecting processing-time latency, queuing times negatively influence event-\-time latency, eventually leading to an incomplete evaluation of the bench\-marked system~\cite{friedrich_coordinated_2017}.

%     %\textbf{Throughput} is \enquote{the number of events that the system can process in a given amount of time}~\cite{karimov_benchmarking_2018}. Further, the authors note that event-time latency and throughput do not necessarily correlate.

%     \textbf{Sustainable Throughput} is therefore defined as the throughput that a system can maintain without systematically increasing the number of elements in the queue before ingestion~\cite{karimov_benchmarking_2018}.
    
%     While mainly targeting benchmnarks that measure the performance of a SPE through latency and throughput, the aforementioned points have implications for the benchmarks that we conduct, so we clearly name the measured values throughout the following section when necessary.
%\section{Nebula Stream}\label{Nebula Stream}
